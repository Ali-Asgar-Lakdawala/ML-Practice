{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": " Support Vector Machine-Airline tweets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ali-Asgar-Lakdawala/ML-Practice/blob/main/Support_Vector_Machine_Airline_tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYsTFWXYjCxF"
      },
      "source": [
        "# <u><b> Objective </b></u>\n",
        "## <b> You are given a data of US Airline tweets and their sentiment. The task is to do sentiment analysis about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\"). </b>\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "## <b>Things to do :</b>\n",
        "* ### Read the tweets.csv data, clean and tokenize the tweets using nltk library.\n",
        "* ### Count vectorize the tweets so that you end up with a sparse matrix (which will be your $X$). \n",
        "* ### You are supposed to build a SVM classifier (a binary classification in fact). Since the data contains three levels of sentiment(positive, negative and neutral), you should remove the sentences which are neutral. Once you do that you will have two classes only (positive and negative). You can set the label of positive tweets to 1 and negative tweets to 0.\n",
        "* ### Once you have built the SVM classifier, evaluate this model across various metrics. Also plot the ROC curve and Precision-Recall curve. Report the areas under these two curves along with other metrics.\n",
        "* ### Perform GridSearch cross validation for various values of $C$ and $gamma$. These will be the hyperparameters which you would play around with.\n",
        "* ### Explain your observations and the underlying reasons for these.\n",
        "* ### Try checking if <code>tfidfvectorizer</code> helps you gain lift in model's performance.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KSvH4wpkFdk",
        "outputId": "e1247277-a1ed-41c7-935e-80bad8c310c5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Ie9ntEkNJH"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YykBT862kL-Q"
      },
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ML/Tweets.csv')"
      ],
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbhbpbFTr6nA"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CdpEjzIm9kh"
      },
      "source": [
        "airlines_names=df.airline.value_counts().index.to_list()"
      ],
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGfj6FlPpUyY"
      },
      "source": [
        "for i,value in enumerate(airlines_names) :\n",
        "  value=value.replace(' ','')\n",
        "  airlines_names[i]=\"@\"+value\n"
      ],
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8msP5TSPot2Q"
      },
      "source": [
        "airlines_names.append('@AmericanAir')"
      ],
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1PF_6R5uYMH",
        "outputId": "6b59071f-f3e6-4303-991f-b2a76f72cb39"
      },
      "source": [
        "airlines_names"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@United',\n",
              " '@USAirways',\n",
              " '@American',\n",
              " '@Southwest',\n",
              " '@Delta',\n",
              " '@VirginAmerica',\n",
              " '@AmericanAir']"
            ]
          },
          "metadata": {},
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-hf5m6DmSMA"
      },
      "source": [
        "df=df.loc[:,['airline_sentiment','text']]"
      ],
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISGRZaGYkiWz"
      },
      "source": [
        "df=df[df['airline_sentiment']!='neutral']"
      ],
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-FboDrtHTp4",
        "outputId": "27176258-9cbf-45ca-a6f4-d95541ec606f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOE9ko8ouokn"
      },
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e04Eh_pZLeye"
      },
      "source": [
        "stop = stopwords.words('english')+airlines_names"
      ],
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6n2MM6DK0m_"
      },
      "source": [
        "df.text = df.text.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
      ],
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgDLu2H-OLor"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hdMg_jQOP8m"
      },
      "source": [
        "vectorizer = TfidfVectorizer(max_df=0.95)\n",
        "X = vectorizer.fit_transform(df.text)"
      ],
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij3l5MO7wZKH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnGZveWBwZHm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsiO_37pwZEu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fhPv_UswZBS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGY36U_FwY5F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjb0u4HcwY2I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm-ET2zrPpJN"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "msg_train,msg_test,label_train,label_test = train_test_split(df.text,df.airline_sentiment,test_size=0.2)"
      ],
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EptnJgdGMUS"
      },
      "source": [
        "train_vectorized = vectorizer.transform(msg_train)\n",
        "test_vectorized = vectorizer.transform(msg_test)"
      ],
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltHiuyb2Gdxg"
      },
      "source": [
        "train_array= train_vectorized.toarray()\n",
        "test_array = test_vectorized.toarray()"
      ],
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "9ae436fe7af2fca2dc7b0fd25d44567c88aa24b8",
        "id": "HLrQuWVyxLYf"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "spam_detect_model = GaussianNB().fit(train_array,label_train)"
      ],
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koOOJdf0FTks"
      },
      "source": [
        "train_preds = spam_detect_model.predict(train_array)\n",
        "test_preds = spam_detect_model.predict(test_array)"
      ],
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "03ac047e8e9e996e7723e901bf00fd7478ab9755",
        "id": "SCJo7wD7xLYg"
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix"
      ],
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mb0ocgK0G8Vk",
        "outputId": "23cc2ac2-6697-43cb-b122-474c19018874"
      },
      "source": [
        "# Confusion matrices for train and test \n",
        "print(confusion_matrix(label_test,test_preds))"
      ],
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1376  493]\n",
            " [ 150  290]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fiFn6Z7vlyC",
        "outputId": "3d6851bf-b81b-4930-e844-c61b5ed0ef82"
      },
      "source": [
        "print(classification_report(label_test,test_preds))"
      ],
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.90      0.74      0.81      1869\n",
            "    positive       0.37      0.66      0.47       440\n",
            "\n",
            "    accuracy                           0.72      2309\n",
            "   macro avg       0.64      0.70      0.64      2309\n",
            "weighted avg       0.80      0.72      0.75      2309\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZFPjPmboB_5",
        "outputId": "86d723db-ad78-430b-de55-632ff4eeec9b"
      },
      "source": [
        "# Print the classification report for train and test\n",
        "print(classification_report(label_test,test_preds))"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.94      0.51      0.66      1843\n",
            "    positive       0.31      0.88      0.46       466\n",
            "\n",
            "    accuracy                           0.58      2309\n",
            "   macro avg       0.63      0.69      0.56      2309\n",
            "weighted avg       0.82      0.58      0.62      2309\n",
            "\n"
          ]
        }
      ]
    }
  ]
}